{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Disaster Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.classification.{RandomForestClassifier, BinaryLogisticRegressionTrainingSummary, RandomForestClassificationModel, LogisticRegression, LogisticRegressionModel, BinaryLogisticRegressionSummary}\n",
    "import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}\n",
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Init / Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PassengerId: string, Survived: string ... 10 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().appName(\"titanic\").getOrCreate() // start the spark session\n",
    "val passengers = spark.read.option(\"header\", \"true\").csv(\"train.csv\").as(\"Passenger\") // read in data set\n",
    "passengers.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count nulls, avg age, do null replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabin has null count of: 687\n",
      "Age has null count of: 177\n",
      "Embarked has null count of: 2\n",
      "Average age is: 29.69911764705882\n"
     ]
    }
   ],
   "source": [
    "val passengers_reduced = passengers.drop(\"PassengerId\", \"Name\", \"Ticket\") // drop the passenger id and the name, as they should not be significant\n",
    "\n",
    "// find null values in a column\n",
    "println(\"Cabin has null count of: \" + passengers_reduced.filter(\"Cabin is null\").count())\n",
    "println(\"Age has null count of: \" + passengers_reduced.filter(\"Age is null\").count())\n",
    "println(\"Embarked has null count of: \" + passengers_reduced.filter(\"Embarked is null\").count())\n",
    "\n",
    "// calculate average for age column\n",
    "val avgAge = passengers_reduced.filter(\"Age is not null\").agg(avg(\"Age\")).collect()(0)(0)\n",
    "println(\"Average age is: \" + avgAge)\n",
    "\n",
    "// fill null values\n",
    "// Replace null age values with the mean age\n",
    "// Replace null ports of embarkation with port \"U\" for unknown\n",
    "// Replace null cabin field with cabin \"U\" for unknown\n",
    "val pnc = passengers_reduced.na.fill(Map(\"Age\" -> avgAge,\n",
    "                                         \"Embarked\" -> \"U\",\n",
    "                                         \"Cabin\" -> \"U\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Fare and Age columns, Index String Columns + Turn into one-hot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// cast the Fare and Age columns to doubles\n",
    "var df = pnc.select(pnc.col(\"*\"),\n",
    "                    pnc(\"Fare\").cast(\"double\").alias(\"FareDbl\"),\n",
    "                    pnc(\"Age\").cast(\"double\").alias(\"AgeDbl\")).drop(\"Fare\", \"Age\")\n",
    "\n",
    "//df.describe(df.drop(\"Sex\", \"Pclass\").columns: _*).show()\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "val encoder = new OneHotEncoder()\n",
    "\n",
    "var column_name = \"\"\n",
    "\n",
    "for ( column_name <- List(\"Sex\", \"Pclass\", \"SibSp\", \"Parch\", \"Cabin\", \"Embarked\") ) {\n",
    "    indexer.setInputCol(column_name).setOutputCol(column_name+\"_ind\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "    \n",
    "    encoder.setInputCol(column_name+\"_ind\").setOutputCol(column_name+\"_vec\")\n",
    "    df = encoder.transform(df).drop(column_name, column_name + \"_ind\")\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble feature vectors and label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val featureAssembler = new VectorAssembler().setInputCols(Array(\"FareDbl\", \"AgeDbl\", \"Sex_vec\", \"Pclass_vec\", \"SibSp_vec\", \"Parch_vec\", \"Embarked_vec\")).setOutputCol(\"features\")\n",
    "\n",
    "indexer.setInputCol(\"Survived\").setOutputCol(\"Survived\"+\"_ind\")\n",
    "df = indexer.fit(df).transform(df).drop(\"Survived\").withColumnRenamed(\"Survived_ind\", \"Survived\")\n",
    "\n",
    "df = featureAssembler.transform(df)\n",
    "df = df.withColumnRenamed(\"Survived\", \"label\")\n",
    "    \n",
    "val lr_model = new LogisticRegression() // Initialize LR model\n",
    "val rf_model = new RandomForestClassifier() // Initialize RF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and fit pipeline (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val generic_pipeline = new Pipeline()\n",
    "val lr_pipeline = generic_pipeline.setStages(Array(lr_model))\n",
    "val lr_fit = lr_pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross-validated accuracy: 0.8068410173417301\n"
     ]
    }
   ],
   "source": [
    "var accuracy_evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "var emptyGrid = new ParamGridBuilder().build()\n",
    "var crossVal = new CrossValidator().setEstimator(lr_pipeline).setEvaluator(accuracy_evaluator).setEstimatorParamMaps(emptyGrid).setNumFolds(5)\n",
    "\n",
    "var cvFit = crossVal.fit(df)\n",
    "var metric = cvFit.avgMetrics(0)\n",
    "println(\"5-fold cross-validated accuracy: \" + metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var lrSummary = lr_fit.stages(0).asInstanceOf[LogisticRegressionModel].summary.asInstanceOf[BinaryLogisticRegressionSummary]\n",
    "val loss = lr_fit.stages(0).asInstanceOf[LogisticRegressionModel].summary.asInstanceOf[BinaryLogisticRegressionTrainingSummary].objectiveHistory\n",
    "var roc = lrSummary.roc\n",
    "var pr = lrSummary.pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "\n",
    "val loss_df = sc.parallelize(loss.zipWithIndex).toDF(\"Loss\", \"Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit pipeline (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rf_pipeline = generic_pipeline.setStages(Array(rf_model))\n",
    "val rf = rf_pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross-validated accuracy: 0.8139402707453315\n"
     ]
    }
   ],
   "source": [
    "var accuracy_evaluator2 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "var emptyGrid2 = new ParamGridBuilder().build()\n",
    "var crossVal2 = new CrossValidator().setEstimator(rf_pipeline).setEvaluator(accuracy_evaluator2).setEstimatorParamMaps(emptyGrid2).setNumFolds(5)\n",
    "\n",
    "var cvFit2 = crossVal2.fit(df)\n",
    "var metric2 = cvFit2.avgMetrics(0)\n",
    "println(\"5-fold cross-validated accuracy: \" + metric2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.11411160350075174,0.08753538928891272,0.41360618625197015,0.17761604620597035,0.027962032058452213,0.018279788287787,0.021293682282097118,0.004843686249781529,0.007663806223508535,0.008284514936144603,0.004745572354877171,0.030004097609481712,0.017526164974931153,0.008442423968666853,0.003178353686799715,0.0012632909760254927,0.001643775011788731,0.02745462947340188,0.01658124029527312,0.00796371636337827])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = rf.stages(0).asInstanceOf[RandomForestClassificationModel]\n",
    "model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run test set through to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabin has null count of: 327\n",
      "Age has null count of: 86\n",
      "Embarked has null count of: 0\n"
     ]
    }
   ],
   "source": [
    "// now to put the test set through the pipeline\n",
    "val passengers_test = spark.read.option(\"header\", \"true\").csv(\"test.csv\").as(\"Passenger\") // read in data set\n",
    "passengers_test.cache()\n",
    "\n",
    "val passengers_reduced = passengers_test.drop(\"Ticket\") // drop the passenger id and the name, as they should not be significant\n",
    "\n",
    "// find null values in a column\n",
    "println(\"Cabin has null count of: \" + passengers_reduced.filter(\"Cabin is null\").count())\n",
    "println(\"Age has null count of: \" + passengers_reduced.filter(\"Age is null\").count())\n",
    "println(\"Embarked has null count of: \" + passengers_reduced.filter(\"Embarked is null\").count())\n",
    "\n",
    "// fill null values\n",
    "// Replace null age values with the mean age\n",
    "// Replace null ports of embarkation with port \"U\" for unknown\n",
    "// Replace null cabin field with cabin \"U\" for unknown\n",
    "val pnc = passengers_reduced.na.fill(Map(\"Age\" -> avgAge,\n",
    "                                         \"Embarked\" -> \"U\",\n",
    "                                         \"Cabin\" -> \"U\",\n",
    "                                         \"Fare\" -> 0,\n",
    "                                         \"Pclass\" -> 0,\n",
    "                                         \"SibSp\" -> 0,\n",
    "                                         \"Parch\" -> 0))\n",
    "                                         \n",
    "// cast the Fare and Age columns to doubles\n",
    "var df = pnc.select(pnc.col(\"*\"),\n",
    "                    pnc(\"Fare\").cast(\"double\").alias(\"FareDbl\"),\n",
    "                    pnc(\"Age\").cast(\"double\").alias(\"AgeDbl\")).drop(\"Fare\", \"Age\")\n",
    "\n",
    "//df.describe(df.drop(\"Sex\", \"Pclass\").columns: _*).show()\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "val encoder = new OneHotEncoder()\n",
    "\n",
    "var column_name = \"\"\n",
    "\n",
    "for ( column_name <- List(\"Sex\", \"Pclass\", \"SibSp\", \"Parch\", \"Cabin\", \"Embarked\") ) {\n",
    "    indexer.setInputCol(column_name).setOutputCol(column_name+\"_ind\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "    \n",
    "    encoder.setInputCol(column_name+\"_ind\").setOutputCol(column_name+\"_vec\")\n",
    "    df = encoder.transform(df).drop(column_name, column_name + \"_ind\")\n",
    "} \n",
    "\n",
    "val featureAssembler = new VectorAssembler().setInputCols(Array(\"FareDbl\", \"AgeDbl\", \"Sex_vec\", \"Pclass_vec\", \"SibSp_vec\", \"Parch_vec\", \"Embarked_vec\")).setOutputCol(\"features\")\n",
    "\n",
    "df = featureAssembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val test_preds = rf.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var preds_kaggle = test_preds.select(\"PassengerId\",\"prediction\").withColumnRenamed(\"Prediction\", \"Survived\").withColumnRenamed(\"PassengerId\", \"PassengerID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val n = preds_kaggle.na.drop()\n",
    "var nnew = n.select(n(\"PassengerId\").alias(\"PassengerID\"),n(\"Survived\").cast(\"Int\"))\n",
    "nnew.coalesce(1).write.option(\"header\", \"true\").csv(path=\"finalpreds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark (Scala 2.11)",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
